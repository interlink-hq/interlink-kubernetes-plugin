[app]
name=interlink-kubernetes-plugin
description=Interlnk Kubernetes Plugin - Offload POD to a remote Kubernetes cluster
version=dev
api_versions=v1
api_docs_path=/docs
# Where the plugin listens for InterLink API sidecar calls.
# Unix socket mode:
socket_address=unix:///var/run/.plugin.sock
socket_port=0
# HTTP mode:
# socket_address=http://0.0.0.0
# socket_port=4000

[log]
level=DEBUG
# dir=/data/logs
rich_enabled=True
requests_enabled=False

[k8s]
# Path to the kubeconfig file to access the remote Kubernetes cluster.
kubeconfig_path=private/k8s/kubeconfig.yaml
# Alternatively, provide the kubeconfig content as a JSON string (escaped)
# Note: convert YAML to JSON: yq '.' ${KUBECONFIG} -c
# kubeconfig={"apiVersion":"v1","clusters":[],"contexts":[],"current-context":"public","kind":"Config","preferences":{},"users":[]}
# Options to set to the underlying python Kubernetes client
# client_configuration={"verify_ssl": true, "ssl_ca_cert": "private/k8s/ca.crt", "cert_file": "private/k8s/client.crt", "key_file": "private/k8s/client.key"}

[offloading]
# Prepend this prefix to the namespace of the offloaded PODs to avoid name clashes
# with existing namespaces in the remote cluster
namespace_prefix=offloading
# Optionally, exclude some namespaces from prepending the prefix, e.g., if you want to offload
# to namespaces with the same name in the remote cluster
# namespace_prefix_exclusions=["kube-system"]
# Optionally, specify node selector and tolerations to be applied to the offloaded PODs
# node_selector={"nvidia/gpu-model": "T4"}
# node_tolerations=[{"key": "nvidia.com/gpu", "operator": "Exists", "effect": "NoSchedule"}]

[tcp_tunnel]
# This feature is deprecated.
# Use the Interlink Network Mesh feature instead of TCP Tunnels.
enabled=False
bastion_namespace=tcp-tunnel
bastion_chart_path=infr/charts/tcp-tunnel/charts/bastion
gateway_host=
gateway_port=
gateway_ssh_private_key=
